{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_ipython().magic(u'matplotlib inline')\n",
    "import matplotlib.pyplot as plt\n",
    "from urllib2 import Request, urlopen, URLError\n",
    "import numpy as np\n",
    "from skimage.util import view_as_windows as vaw\n",
    "import tflearn\n",
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "import tensorflow as tf\n",
    "from scipy.stats import mode\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kw = 'homeobox'  # the keyword to search uniprot for\n",
    "test_len = 100  # the length to cut each sequence up into\n",
    "sequences = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# homeobox\n",
    "sequences['homeobox'] = ['r', 'r', 'r', 'k', 'r', 't', 'a', 'y',\n",
    "                         't', 'r', 'y', 'q','l', 'l', 'e', 'l', 'e',\n",
    "                         'k', 'e', 'f', 'h', 'f', 'n', 'r', 'y', 'l',\n",
    "                         't', 'r', 'r', 'r', 'r', 'i', 'e', 'l', 'a',\n",
    "                         'h', 's', 'l', 'n', 'l', 't', 'e','r', 'h',\n",
    "                         'i', 'k', 'i', 'w', 'f', 'q', 'n', 'r', 'r',\n",
    "                         'm', 'k', 'w', 'k', 'k', 'e', 'n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# kw-0440 - LIM\n",
    "sequences['KW-0440'] = ['c', 'x', 'x', 'x', 'x', 'c', 'x', 'x',\n",
    "                        'x', 'x', 'x', 'x', 'x', 'x', 'x','x', 'x',\n",
    "                        'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'w', 'h',\n",
    "                        'x', 'x', 'x', 'x', 'c', 'f', 'x', 'c', 'x', 'x',\n",
    "                        'x', 'x', 'c', 'x', 'x', 'x', 'x', 'x', 'x', 'x',\n",
    "                        'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x', 'x',\n",
    "                        'x', 'x', 'x', 'c', 'x', 'x', 'x', 'x', 'c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# kw-0501\n",
    "consensus = 'MQAEILLTLKLQQKLFADPRRISLLKHIALSGSISQGAKDAGISYKSAWDAINEMNQLSEHILVERATGGKGGGGAVLTRYGQRLIQLYDLLAQIQQKAFDVLSDDDALPLNSLLAAISRFSLQTSARNQWFGTITARDHDDVQQHVDVLLADGKTRLKVAITAQSGARLGLDEGKEVLILLKAPWVGITQDEAVAQNADNQLPGIISHIERGAEQCEVLMALPDGQTLCATVPVNEATSLQQGQNVTAYFNADSVIIATLC'\n",
    "sequences['KW-0501'] = map(lambda x:x.lower(), list(consensus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def letter2num(c, length):\n",
    "    '''Assigns each letter in the amino acid code c\n",
    "       a unique integer and chops the sequence into\n",
    "       shorter sequences.'''\n",
    "          \n",
    "    try:\n",
    "        len(c[0])\n",
    "        X = np.zeros([0, length])\n",
    "        \n",
    "        for i in xrange(len(c) - 1):\n",
    "            if len(c[i]) >= length:\n",
    "                x = []\n",
    "                \n",
    "                for j in xrange(len(c[i])):\n",
    "                    x.append(max(ord(c[i][j])-97, 0))\n",
    "                    \n",
    "                X = np.concatenate((X, vaw(np.asarray(x), (length,))), 0)\n",
    "                \n",
    "    except TypeError:\n",
    "        X = np.zeros([len(c)])\n",
    "        for i in xrange(len(c)):\n",
    "            X[i] = (ord(c[i])-97)\n",
    "        \n",
    "        if length is not None:\n",
    "            X = vaw(X, (length, ))\n",
    "            \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "consensus = sequences[kw]\n",
    "c_length = len(consensus)  # length of the respective consensus sequence\n",
    "c = letter2num(consensus, len(consensus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_uniprot_data(query, limit):\n",
    "    '''Goes to the uniprot website and searches for \n",
    "       data with the keyword query. Returns the data \n",
    "       found up to limit elements.'''\n",
    "    \n",
    "    url1 = 'http://www.uniprot.org/uniprot/?query'\n",
    "    url2 = '&columns=sequence&format=tab&limit='+str(limit)\n",
    "    query_complete = url1 + query + url2\n",
    "    request = Request(query_complete)\n",
    "    response = urlopen(request)\n",
    "    data = response.read()\n",
    "    data = data.split('\\n')\n",
    "    data = data[1:]\n",
    "    \n",
    "    return map(lambda x:x.lower(),data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LSTM(net, d_prob):\n",
    "    '''The LSTM model used for training'''\n",
    "    \n",
    "    net = tflearn.embedding(net, input_dim=25, output_dim=25)\n",
    "    net = tflearn.lstm(net, 300, dropout=d_prob, dynamic=False, return_seq=True)\n",
    "    # net = tflearn.lstm(net, 140, dropout=0.5, dynamic=True, return_seq=True)\n",
    "    net = tflearn.lstm(net, 300)\n",
    "    net = tflearn.fully_connected(net, 150, activation='tanh')\n",
    "    net = tflearn.fully_connected(net, 2, activation='softmax')\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def num2letter(seq):\n",
    "    '''Takes in a seq of amino acids composed of numbers \n",
    "       and transforms them into the respective letters.'''\n",
    "    \n",
    "    letters = []\n",
    "    \n",
    "    for i in xrange(len(seq)):\n",
    "        letters.append(chr(int(seq[i]+97)))\n",
    "        \n",
    "    return letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through the training procedure with different sub-sequence\n",
    "# lengths\n",
    "for i in xrange(60, 151, 5):\n",
    "    \n",
    "    # reset the tensorflow graph each time we change the \n",
    "    # sequence size\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # get the data from the uniprot webpage\n",
    "    # in this case, were pulling 1300 proteins \n",
    "    # in each category\n",
    "    x1 = get_uniprot_data('=' + kw, 2000)\n",
    "    x0 = get_uniprot_data('=NOT+' + kw, 2000)\n",
    "    \n",
    "    # Transform each letter in the amino acid sequence\n",
    "    # into a unique integer\n",
    "    x1=letter2num(x1, i)\n",
    "    x0=letter2num(x0, i)\n",
    "    data_length = x0.shape[0]\n",
    "\n",
    "    # make the number of examples equal for each group\n",
    "    # by shortening x1, which is the one with more data\n",
    "    x1=x1[:x0.shape[0], :]\n",
    "    \n",
    "    # put all examples from both groups into one variable X\n",
    "    X = np.concatenate((x1, x0), 0)\n",
    "    \n",
    "    # Create labels and make them into one-hot vectors with\n",
    "    # two dimensions\n",
    "    Y = np.zeros([x1.shape[0]+x0.shape[0], ])\n",
    "    Y[:x1.shape[0]] = 1.0\n",
    "    Y = to_categorical(Y, 2)\n",
    "\n",
    "    # send the input placeholder to the lstm \n",
    "    net = tflearn.input_data([None, i])\n",
    "    net = LSTM(net, 0.5) # get the output of the network\n",
    "    \n",
    "    # choose the backprop algorithm, learning rate, and\n",
    "    # objective function\n",
    "    net = tflearn.regression(net, optimizer='adam',\n",
    "                             learning_rate=0.00001,\n",
    "                             loss='categorical_crossentropy')\n",
    "    \n",
    "    # instantiate the model\n",
    "    model = tflearn.DNN(net, tensorboard_verbose=1)\n",
    "    \n",
    "    # call the training method with its parameters\n",
    "    model.fit(X, Y, \n",
    "          n_epoch=10, \n",
    "          validation_set=0.25, \n",
    "          shuffle=True, \n",
    "          show_metric=True, \n",
    "          batch_size=70,\n",
    "          snapshot_step=10000,\n",
    "          run_id='ProteinNet_' + str(i) + '_' + str(data_length) + '_LSTM_' + str(np.random.randint(1, 10**4)))\n",
    "    \n",
    "    model.save('lstm' + str(i) + kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send the input placeholder to the lstm \n",
    "net = tflearn.input_data([None, test_len])\n",
    "net = LSTM(net, 1.0) # get the output of the network\n",
    "\n",
    "# instantiate the model\n",
    "model = tflearn.DNN(net, tensorboard_verbose=1)\n",
    "\n",
    "# load the saved model you want to test\n",
    "model.load('lstm' + str(test_len), weights_only=True)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_test = 2000 # number of proteins to test\n",
    "X1 = get_uniprot_data('=' + kw, num_test)\n",
    "best = np.zeros([0, test_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in xrange(num_test):\n",
    "    x1 = X1[i]\n",
    "    \n",
    "    if len(x1) > test_len:\n",
    "        x1 = letter2num(x1, test_len)\n",
    "        x1out = np.asarray(model.predict(x1))\n",
    "        \n",
    "        if np.amin(x1out[:, 1])-np.amax(x1out[:, 0]) > 0.5:\n",
    "            print(np.mean(x1out[:, 1])-np.mean(x1out[:, 0]))\n",
    "            best_ind = np.argmax(x1out[:, 1])\n",
    "\n",
    "            # plot the activation of each output node for each seq. fragment\n",
    "            fig = plt.figure()\n",
    "            a0 = fig.add_subplot(111)\n",
    "            a0.set_ylabel('activation of output layer')\n",
    "            a0.set_xlabel('sequence fragment')\n",
    "            a0.plot(x1out[:, 0], label = 'node 0')\n",
    "            a0.plot(x1out[:, 1], label='node 1')\n",
    "            a0.scatter(best_ind, 0)\n",
    "            plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "            plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check the consensus and pad it to same size as network input\n",
    "if c.shape != (test_len,):\n",
    "    c = c[0, :]\n",
    "    c = np.pad(c, ((test_len-c_length)/2, (test_len-c_length)/2),\n",
    "               'constant', constant_values=-1.)\n",
    "    print(c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run the consensus sequence back through the model to\n",
    "# see the activity of the output layer\n",
    "output = model.predict(c[None, :])\n",
    "print(np.amax(output))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
